[
  {
    "objectID": "posts/stats/MixedEffectsModels.html",
    "href": "posts/stats/MixedEffectsModels.html",
    "title": "Linear mixed effect modesl",
    "section": "",
    "text": "Welcome to this introduction to Linear Mixed-effects Models (LMM)!! In this tutorial we will use R to run some simple LMM and we will try to understand together how to leverage these model for our analysis.\n        LMMs are amazing tools that have saved our asses countless times during our PhDs and Postdocs. They'll probably continue to be our trusty companions forever.\nThis tutorial introduces the statistical concept of Hierarchical Modeling, often called Mixed Effects Modeling. This approach shines when dealing with nested data—situations where observations are grouped in meaningful ways, like students within schools or measurements across individuals.\nSounds like a mouthful, right? Don’t worry! Let’s kick things off with something a little more fun: Simpson’s Paradox.\nSimpson’s Paradox is a statistical head-scratcher. It’s when a trend you see in your data suddenly flips—or even vanishes—when you split the data into groups. Think of it as the “oops” moment in data analysis when you realize you missed something big. Ready to see it in action? Let’s dive in!\nImagine we’re looking at how years of experience impact salary at a university. Here’s some simulated data to make it fun.\nCodelibrary(easystats)\nlibrary(tidyverse)\nlibrary(patchwork)\nset.seed(1234)\ndata &lt;- simulate_simpson(n = 10, groups = 5, r = 0.5,difference = 1.5) %&gt;% \n  mutate(V2= (V2 +abs(min(V2)))*10000) %&gt;% \n  rename(Department = Group)\n\n# Lookup vector: map old values to new ones\nlookup &lt;- c(G_1 = \"Informatics\", G_2 = \"English\", \n            G_3 = \"Sociology\", G_4 = \"Biology\", G_5 = \"Statistics\")\n\n# Replace values using the lookup vector\ndata$Department &lt;- lookup[as.character(data$Department)]\n\n\none = ggplot(data, aes(x = V1, y = V2)) +\n  geom_point()+\n  geom_smooth(method = 'lm')+\n  labs(y='Salary', x='Year of experience', title = \"A. Linear model\")+\n  theme_bw(base_size = 20)\n\ntwo = ggplot(data, aes(x = V1, y = V2)) +\n  geom_point(aes(color = Department)) +\n  geom_smooth(aes(color = Department), method = \"lm\", alpha = 0.3) +\n  geom_smooth(method = \"lm\", alpha = 0.3)+\n  labs(y='Salary', x='Year of experience', title = \"B. Linear model acounting for hierarchical structure\")+\n  theme_bw(base_size = 20)+\n  theme(legend.position = 'bottom')\n\n(one / two)\nTake a look at the first plot. Whoa, wait a minute—this says the more years of experience you have, the less you get paid! What kind of backwards world is this? Before you march into HR demanding answers, let’s look a little closer.\nNow, check out the second plot. Once we consider the departments—Informatics, English, Sociology, Biology, and Statistics—a different story emerges. Each department shows a positive trend between experience and salary. In other words, more experience does mean higher pay, as it should!\nSo what happened? The first plot ignored the hierarchical structure of the data. By lumping all departments together, it completely missed the real relationship hiding in plain sight. This is why Hierarchical Modeling is so powerful—it helps us avoid embarrassing statistical blunders like this one. It allows us to correctly analyze data with nested structures and uncover the real patterns.\nNow, this example is a bit of an extreme case. In real life, you’re less likely to find such wildly opposite effects. However, the influence of grouping structures on your analysis is very real—and often subtle. Ignoring them can lead to misleading conclusions.\nReady to explore how Mixed Effects Modeling helps us account for these nested structures? Let’s dive in and get hands-on!",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "posts/stats/MixedEffectsModels.html#settings-and-data",
    "href": "posts/stats/MixedEffectsModels.html#settings-and-data",
    "title": "Linear mixed effect modesl",
    "section": "Settings and data",
    "text": "Settings and data\nIn this section, we’ll load up the libraries and the data we’ll use for this tutorial. The data we’ll work with is simulated to resemble real-world scenarios and you can download the dataset from here:\nDownload data\n\nlibrary(lme4)\nlibrary(lmerTest)\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nThe lme4 package is the go-to library for running Linear Mixed Models (LMM) in R. To make your life easier, there’s also the lmerTest package, which enhances lme4 by allowing you to extract p-values and providing extra functions to better understand your models. In my opinion, you should always use lmerTest alongside lme4—it just makes everything smoother!\nTo run our Linear Mixed Effects Model, these are the key packages we’ll use. On top of that, the tidyverse suite will help with data wrangling and visualization, while easystats will let us easily extract and summarize the important details from our models. Let’s get started!",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "posts/stats/MixedEffectsModels.html#the-data",
    "href": "posts/stats/MixedEffectsModels.html#the-data",
    "title": "Linear mixed effect modesl",
    "section": "The data",
    "text": "The data\n\ndb = read.csv('../../resources/simulation/simulated_data.csv')\ndb = db %&gt;% filter(subject_id &lt;10)\ndb$subject_id = factor(db$subject_id) # make sure subject_id is a factor\n\nhead(db)\n\n  subject_id condition trial_number performance\n1          1    Hammer            1    548.5242\n2          1    Hammer            2    557.3464\n3          1    Hammer            3    574.2554\n4          1    Hammer            4    589.2188\n5          1    Hammer            5    601.3519\n6          1    Hammer            6    619.4832\n\n\nThis dataset contains several columns. The performance column holds our primary variable of interest, the performance score. The condition column tells us the experimental condition under which the data was collected, while trial_number marks the specific trial, and subject_id gives each participant a unique identifier.\nBefore diving into modeling, it’s always a good idea to visualize your data first. So, let’s use ggplot2 (a package from the tidyverse family) to get a quick look at our data. This will help us understand the structure and spot any patterns or oddities right away. Let’s do it!\n\nCodeggplot(db, aes(x= trial_number, y = performance, shape = condition))+\n  geom_point(position= position_jitter(width=0.2))+\n  theme_bw(base_size = 20)+\n  labs(y='Performance', x= '# trial')+\n  scale_x_continuous(breaks = 0:10)\n\n\n\n\n\n\n\nLooking at the plot, there seems to be a small positive trend, but no real difference between the two conditions—hammer and spoon. Also, there’s definitely some weird data pattern, especially near the top. Weird, right?\nLet’s dive in and analyze this with a simple linear model. We’ll start by fitting a model that predicts the performance score based on the condition (hammer or spoon).",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "posts/stats/MixedEffectsModels.html#linear-model",
    "href": "posts/stats/MixedEffectsModels.html#linear-model",
    "title": "Linear mixed effect modesl",
    "section": "Linear model",
    "text": "Linear model\n\n\n\n\n\n\nWarning\n\n\n\nThroughout this section, and in the tutorial as a whole, we’ll assume you have some familiarity with linear models. You don’t need to be an expert, but we will use terminology and concepts related to linear modeling. Because linear mixed-effects models are simply an extension of linear models, if you aren’t already comfortable with them, I recommend reviewing the basics before continuing.\n\n\nHere we run the linear model:\n\nLinear_mod = lm(performance ~ condition * trial_number, db)\nsummary(Linear_mod)\n\n\nCall:\nlm(formula = performance ~ condition * trial_number, data = db)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-189.83  -44.25  -16.71   63.12  172.09 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  417.333     21.357  19.541  &lt; 2e-16 ***\nconditionSpoon                16.593     30.203   0.549    0.583    \ntrial_number                  15.184      3.442   4.411 1.79e-05 ***\nconditionSpoon:trial_number   -7.599      4.868  -1.561    0.120    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 93.79 on 176 degrees of freedom\nMultiple R-squared:  0.1354,    Adjusted R-squared:  0.1207 \nF-statistic: 9.188 on 3 and 176 DF,  p-value: 1.113e-05\n\n\nWe won’t delve into the details of the model results in this tutorial, but from the summary, it’s clear that the model reflects what we saw in the plot: performance significantly increases over time, but there’s no clear difference between conditions. Now, let’s visualize it.\n\nCodel_pred = get_datagrid(Linear_mod, by= 'trial_number')\nl_pred = bind_cols(l_pred, as.data.frame(get_predicted(l_pred, Linear_mod, ci=T)))\n\nggplot(l_pred, aes(x= trial_number, y= Predicted))+\n    geom_point(data = db, aes(y= performance, shape= condition), position= position_jitter(width=0.2))+\n    geom_line(lwd=1.5, color= 'blue')+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE), color= 'transparent', alpha=0.4)+\n    labs(y='Predicted performance', x='# trial')+\n    theme_bw(base_size = 20)\n\n\n\n\n\n\n\nThe linear model confirmed what we observed in the data: performance increases over time, with no clear difference between conditions. But remember, this is a tutorial about mixed-effects models, so there’s something more to explore!",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "posts/stats/MixedEffectsModels.html#mixed-effects",
    "href": "posts/stats/MixedEffectsModels.html#mixed-effects",
    "title": "Linear mixed effect modesl",
    "section": "Mixed Effects",
    "text": "Mixed Effects\nRandom Intercept\nAlright, let’s start with Random Intercepts! What are they? Well, the name gives it away—they’re just intercepts…but with a twist! 🤔\nIf you recall your knowledge of linear models, you’ll remember that each model has one intercept—the point where the model crosses the y-axis (when x=0).\nBut what makes random intercepts special? They allow the model to have different intercepts for each grouping variable—in this case, the subjects. This means we’re letting the model assume that each subject may have a slightly different baseline performance.\nHere’s the idea:\n\nOne person might naturally be a bit better.\nSomeone else could be slightly worse.\nAnd me? Well, let’s just say I’m starting from rock bottom.\n\nHowever, even though we’re starting from different baselines, the rate of improvement over trials can still be consistent across subjects.\nThis approach helps us capture variation in the starting performance, acknowledging that people are inherently different but might still follow a similar overall pattern of improvement. It’s a simple yet powerful way to model individual differences!\nNow, let’s look at how to include this in our mixed model.\nModel\nTo run a linear mixed-effects model, we’ll use the lmer function from the lme4 package. it Functions very similarly to the lm function we used before: you pass a formula and a dataset, but with one important addition: specifying the random intercept.\nThe formula is nearly the same as a standard linear model, but we include (1|subject_id) to tell the model that each subject should have its own unique intercept. This accounts for variations in baseline performance across individuals.\n\nIntercept_mod =lmer(performance ~ condition * trial_number + (1|subject_id ), db)\nsummary(Intercept_mod)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ condition * trial_number + (1 | subject_id)\n   Data: db\n\nREML criterion at convergence: 1439.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5844 -0.5223 -0.0030  0.5732  3.3163 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n subject_id (Intercept) 9531     97.63   \n Residual                132     11.49   \nNumber of obs: 180, groups:  subject_id, 9\n\nFixed effects:\n                            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)                 417.3326    32.6469   8.0925  12.783 1.19e-06 ***\nconditionSpoon               16.5930     3.7003 168.0000   4.484 1.35e-05 ***\ntrial_number                 15.1836     0.4217 168.0000  36.006  &lt; 2e-16 ***\nconditionSpoon:trial_number  -7.5991     0.5964 168.0000 -12.742  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cndtnS trl_nm\nconditinSpn -0.057              \ntrial_numbr -0.071  0.627       \ncndtnSpn:t_  0.050 -0.886 -0.707\n\n\nWow! Now the model is showing us something new compared to the simple linear model. We observe an interaction between condition and trial number. By letting the intercept vary for each subject, the model is able to capture nuances in the data that a standard linear model might miss.\nTo understand this interaction, let’s plot it and see how performance changes across trials for each condition.\n\nCodei_pred = bind_cols(db, as.data.frame(get_predicted(Intercept_mod, ci=T)))\n\nggplot(i_pred, aes(x= trial_number, y= Predicted, color= subject_id, shape = condition))+\n    geom_point(data = db, aes(y= performance, color= subject_id), position= position_jitter(width=0.2))+\n    geom_line()+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, fill = subject_id),color= 'transparent', alpha=0.1)+\n    labs(y='Predicted performance', x='# trial')+\n    theme_bw(base_size = 20)+\nfacet_wrap(~condition)\n\n\n\n\n\n\n\nNow, you might be thinking, “This looks interesting, but my plot is going to be a mess with all these individual estimates!” Well, don’t worry! While what we’ve plotted is how the data is modeled by our mixed-effects model, the random effects are actually used to make more accurate estimates—but the model still returns an overall estimate.\nThink of it like this: the random effects allow the model to account for individual differences between subjects. But instead of just showing all the individual estimates in the plot, the model takes these individual estimates for each subject and returns the average of these estimates to give you a cleaner, more generalizable result.\nwe can plot the actual estimate of the model:\n\nCodei_pred = get_datagrid(Intercept_mod, include_random = F)\ni_pred = bind_cols(i_pred, as.data.frame(get_predicted(i_pred, Intercept_mod, ci=T)))\n\nggplot(i_pred, aes(x= trial_number, y= Predicted))+\n    geom_point(data = db, aes(y= performance, color= subject_id, shape = condition), position= position_jitter(width=0.2))+\n    geom_line(aes(group= condition),color= 'blue')+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, group= condition),color= 'transparent', alpha=0.1)+\n    labs(y='Predicted performance', x='# trial')+\n    theme_bw(base_size = 20)+\n  facet_wrap(~condition)\n\n\n\n\n\n\n\nSlope\nCoool!!!!!!! So far, we’ve modeled a different intercept for each subject, which lets each subject have their own baseline level of performance. But here’s the catch: our model assumes that everyone improves over the trials in exactly the same way, with the same slope. That doesn’t sound quite right, does it? We know that some people may get better faster than others, or their learning might follow a different pattern.\nModel\nThis is where we can model random slopes to capture these individual differences in learning rates. By adding (0 + trial_number | subject_id), we’re telling the model that while the intercept (starting point) is the same for everyone, the rate at which each subject improves (the slope) can vary.\nThis way, we’re allowing each subject to have their own slope in addition to their own intercept, making the model more flexible and reflective of real-world variations in learning!\n\nSlope_mod =lmer(performance ~ condition * trial_number + (0+trial_number|subject_id ), db)\nsummary(Slope_mod)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ condition * trial_number + (0 + trial_number |  \n    subject_id)\n   Data: db\n\nREML criterion at convergence: 1882.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1805 -0.4502 -0.0398  0.3804  2.9272 \n\nRandom effects:\n Groups     Name         Variance Std.Dev.\n subject_id trial_number  198.1   14.07   \n Residual                1862.7   43.16   \nNumber of obs: 180, groups:  subject_id, 9\n\nFixed effects:\n                            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)                  417.333      9.828 168.000  42.464  &lt; 2e-16 ***\nconditionSpoon                16.593     13.899 168.000   1.194 0.234217    \ntrial_number                  15.184      4.952   9.685   3.066 0.012353 *  \nconditionSpoon:trial_number   -7.599      2.240 168.000  -3.392 0.000864 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cndtnS trl_nm\nconditinSpn -0.707              \ntrial_numbr -0.284  0.200       \ncndtnSpn:t_  0.627 -0.886 -0.226\n\n\nThe results aren’t too different from the intercept-only model, but let’s take a closer look at what we’ve actually modeled.\n\nCodes_pred = bind_cols(db, as.data.frame(get_predicted(Slope_mod, ci=T)))\n\nggplot(s_pred, aes(x= trial_number, y= Predicted, color= subject_id, shape = condition))+\n    geom_point(data = db, aes(y= performance, color= subject_id), position= position_jitter(width=0.2))+\n    geom_line()+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, fill = subject_id),color= 'transparent', alpha=0.1)+\n    labs(y='Predicted performance', x='# trial')+\n    theme_bw(base_size = 20)+\nfacet_wrap(~condition)\n\n\n\n\n\n\n\nIntercept + Slope\nThat plot does look nuts, and it’s a clear signal that something is off. Why? Because by modeling only the random slopes while keeping the intercepts fixed, we’re essentially forcing all subjects to start from the same baseline. That’s clearly unrealistic for most real-world data.\nIn real life, the intercept and slope often go hand-in-hand for each subject.\nModel\nTo make the model more realistic, we can model both the random intercept and the random slope together. We simply modify the random effects part of the formula to (trial_number | subject_id).\nNow, we are telling the model to estimate both a random intercept (baseline performance) and a random slope (rate of improvement). This captures the full variability in how each subject learns over time!\n\nInterceptSlope_mod =lmer(performance ~ condition * trial_number + (trial_number|subject_id ), db)\nsummary(InterceptSlope_mod)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ condition * trial_number + (trial_number | subject_id)\n   Data: db\n\nREML criterion at convergence: 1198.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0164 -0.5921  0.0796  0.5935  2.8312 \n\nRandom effects:\n Groups     Name         Variance Std.Dev. Corr\n subject_id (Intercept)  9002.77  94.883       \n            trial_number   13.56   3.682   0.03\n Residual                  25.52   5.052       \nNumber of obs: 180, groups:  subject_id, 9\n\nFixed effects:\n                            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)                 417.3326    31.6486   8.0100   13.19 1.03e-06 ***\nconditionSpoon               16.5930     1.6269 160.0003   10.20  &lt; 2e-16 ***\ntrial_number                 15.1836     1.2412   8.1812   12.23 1.53e-06 ***\nconditionSpoon:trial_number  -7.5991     0.2622 160.0003  -28.98  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cndtnS trl_nm\nconditinSpn -0.026              \ntrial_numbr  0.027  0.094       \ncndtnSpn:t_  0.023 -0.886 -0.106\n\n\nNow, let’s visualize how the model is modeling the data:\n\nCodeis_pred = bind_cols(db, as.data.frame(get_predicted(InterceptSlope_mod, ci=T)))\n\nggplot(is_pred, aes(x= trial_number, y= Predicted, color= subject_id, shape = condition))+\n    geom_point(data = db, aes(y= performance, color= subject_id), position= position_jitter(width=0.2))+\n    geom_line()+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, fill = subject_id),color= 'transparent', alpha=0.1)+\n    labs(y='Predicted performance', x='# trial')+\n    theme_bw(base_size = 20)+\nfacet_wrap(~condition)\n\n\n\n\n\n\n\nWhile is not super apparent from the data you can see that different subject have different slopes menaing that they all not grow at the same rate",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "posts/stats/MixedEffectsModels.html#summary-of-mixed-models",
    "href": "posts/stats/MixedEffectsModels.html#summary-of-mixed-models",
    "title": "Linear mixed effect modesl",
    "section": "Summary of mixed models",
    "text": "Summary of mixed models\nNow that we’ve seen how to run mixed-effects models, it’s time to focus on interpreting the summary output. While we’ve been building models, we haven’t delved into what the summary actually tells us or which parts of it deserve our attention. Let’s fix that!\nTo start, we’ll use our final model and inspect its summary. This will give us a chance to break it down step by step and understand the key information it provides. Here’s how to check the summary:\n\nsummary(InterceptSlope_mod)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ condition * trial_number + (trial_number | subject_id)\n   Data: db\n\nREML criterion at convergence: 1198.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0164 -0.5921  0.0796  0.5935  2.8312 \n\nRandom effects:\n Groups     Name         Variance Std.Dev. Corr\n subject_id (Intercept)  9002.77  94.883       \n            trial_number   13.56   3.682   0.03\n Residual                  25.52   5.052       \nNumber of obs: 180, groups:  subject_id, 9\n\nFixed effects:\n                            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)                 417.3326    31.6486   8.0100   13.19 1.03e-06 ***\nconditionSpoon               16.5930     1.6269 160.0003   10.20  &lt; 2e-16 ***\ntrial_number                 15.1836     1.2412   8.1812   12.23 1.53e-06 ***\nconditionSpoon:trial_number  -7.5991     0.2622 160.0003  -28.98  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cndtnS trl_nm\nconditinSpn -0.026              \ntrial_numbr  0.027  0.094       \ncndtnSpn:t_  0.023 -0.886 -0.106\n\n\nThe Random effects section in the model summary shows how variability is accounted for by the random effects. The Groups column indicates the grouping factor (e.g., subject), while the Name column lists the random effects (e.g., intercept and slope). The Variance column represents the variability for each random effect—higher values indicate greater variation in how the effect behaves across groups. The Std.Dev. column is simply the standard deviation of the variance, showing the spread in the same units as the data.\nThe Corr column reflects the correlation between random effects, telling us whether different aspects of the data (e.g., intercepts and slopes) tend to move together. A negative correlation would suggest that higher intercepts (starting points) are associated with smaller slopes (slower learning rates), while a positive correlation would suggest the opposite.\nThe Residual section shows the unexplained variability after accounting for the fixed and random effects.\nThe key takeaway here is that random effects capture the variability in the data that can’t be explained by the fixed effects alone. If the variance for a random effect is low, it suggests the random effect isn’t adding much to the model and may be unnecessary. On the other hand, high variance indicates that the random effect is important for capturing group-level differences and improving the model’s accuracy.",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "posts/stats/MixedEffectsModels.html#model-comparison",
    "href": "posts/stats/MixedEffectsModels.html#model-comparison",
    "title": "Linear mixed effect modesl",
    "section": "Model comparison",
    "text": "Model comparison\nBut how can we be sure the random effects are helping our model? One of the easiest ways is to check the variance explained by the random effects. As we said if the variance related to the random effects is too small, it probably isn’t contributing much to the model. If it’s high, it’s likely helping the model by capturing important variability in the data.\nAnother method is to compare the performance of different models. One of the best indices for this is the Akaike Information Criterion (AIC). AIC gives a relative measure of how well a model fits the data, while penalizing the number of parameters in the model. Lower AIC values indicate better models, as they balance goodness-of-fit with model complexity.\nYou can compare the AIC of different models using the following:\n\ncompare_performance(Linear_mod, Intercept_mod, Slope_mod, InterceptSlope_mod, metrics='AIC')\n\n# Comparison of Model Performance Indices\n\nName               |           Model |  AIC (weights)\n-----------------------------------------------------\nLinear_mod         |              lm | 2151.5 (&lt;.001)\nIntercept_mod      | lmerModLmerTest | 1463.2 (&lt;.001)\nSlope_mod          | lmerModLmerTest | 1913.9 (&lt;.001)\nInterceptSlope_mod | lmerModLmerTest | 1226.0 (&gt;.999)\n\n\nAs you can see, the best model based on AIC is the one with both intercept and slope. This is a good way to check if and which random effect structure is necessary for our model.\n\n\n\n\n\n\nWarning\n\n\n\nNever decide if your random effect structure is good by just looking at p-values! P-values are not necessarily related to how well the model fits your data. Always use model comparison and fit indices like AIC to guide your decision.",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "posts/stats/MixedEffectsModels.html#formulary",
    "href": "posts/stats/MixedEffectsModels.html#formulary",
    "title": "Linear mixed effect modesl",
    "section": "Formulary",
    "text": "Formulary\nIn this tutorial, we introduced linear mixed-effects models. However, these models can be far more versatile and complex than what we’ve just explored. The lme4 package allows you to specify various models to suit diverse research scenarios. While we won’t dive into every possibility, here’s a handy reference for the different random effects structures you can specify\n\n\nFormula\nDescription\n\n\n\n(1|s)\nRandom intercepts for unique level of the factor s.\n\n\n(1|s) + (1|i)\nRandom intercepts for each unique level of s and for each unique level of i.\n\n\n(1|s/i)\nRandom intercepts for factor s and i, where the random effects for i are nested in s. This expands to (1|s) + (1|s:i), i.e., a random intercept for each level of s, and each unique combination of the levels of s and i. Nested random effects are used in so-called multilevel models. For example, s might refer to schools, and i to classrooms within those schools.\n\n\n(a|s)\nRandom intercepts and random slopes for a, for each level of s. Correlations between the intercept and slope effects are also estimated. (Identical to (a*b|s).)\n\n\n(a*b|s)\nRandom intercepts and slopes for a, b, and the a:b interaction, for each level of s. Correlations between all the random effects are estimated.\n\n\n(0+a|s)\nRandom slopes for a for each level of s, but no random intercepts.\n\n\n(a||s)\nRandom intercepts and random slopes for a, for each level of s, but no correlations between the random effects (i.e., they are set to 0). This expands to: (0+a|s) + (1|s).",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "posts/general/GithubDesktop.html",
    "href": "posts/general/GithubDesktop.html",
    "title": "Github Desktop",
    "section": "",
    "text": "So, you’ve started coding and your friend or supervisor suggested creating a repository for your work? Sounds like a smart move! In this tutorial, we’ll explore how to use one of the simplest tools for managing repositories and code on GitHub: GitHub Desktop.",
    "crumbs": [
      "General",
      "Github Desktop"
    ]
  },
  {
    "objectID": "posts/general/GithubDesktop.html#github-desktop-your-gateway-to-version-control",
    "href": "posts/general/GithubDesktop.html#github-desktop-your-gateway-to-version-control",
    "title": "Github Desktop",
    "section": "Github desktop: your gateway to version control",
    "text": "Github desktop: your gateway to version control\nThere are several ways to interact with GitHub and manage your repositories: git command line, GitKraken, GitHub Desktop, or the website interface. For beginners and those seeking simplicity, GitHub Desktop is an excellent choice.\nGitHub Desktop offers a user-friendly approach to handle your basic version control needs. It’s straightforward enough for newcomers yet capable enough for most projects. You can easily clone repositories, commit changes, and manage your work without diving you crazy with complex commands.\nWhile GitHub Desktop is perfect for getting started and handling everyday tasks, more advanced users working on complex projects might eventually explore more powerful tools. For now, let’s focus on GitHub Desktop - it’s the ideal balance of functionality and ease of use for most us.\nSo go download it and get back here!! Once installed you need to log in and then you are ready for your first repository.",
    "crumbs": [
      "General",
      "Github Desktop"
    ]
  },
  {
    "objectID": "posts/general/GithubDesktop.html#make-your-first-repository",
    "href": "posts/general/GithubDesktop.html#make-your-first-repository",
    "title": "Github Desktop",
    "section": "Make your first repository",
    "text": "Make your first repository\nOk now we are in!! Let’s create a new repository. You can click on “File” and then “New repository…” (you can also just press CTRL + N).\n\n\n\n\n\nNow let’s go through the process of setting up your repository:\n\nName: Give a name to your repository. In this example, it’s simply called “Test.”\nDescription: This is, as it sounds, a brief description of what your repository is all about.\nLocal path: This is essentially where GitHub Desktop will create the folder for your repository. In this case, it will create a Test folder on the desktop (as mentioned at the bottom of the interface).\nREADME check-box: This allows you to include a README file in the folder that will be created. We’ll discuss the README file later, but for now, know that it’s a file using markdown where you can add information about your repository. It’s the first interface between your code and someone checking your repository. It’s advisable to always have a README file. At worst, you can keep it empty initially.\nGit ignore: This is a file where you can specify files that GitHub won’t track changes for. For simple projects, you often won’t need it.\nLicense: This allows you to specify the license you want to use for your code. While it’s often overlooked, it’s worth considering how you want your code to be used by others.\n\nFor our first repository these settings will work fine. Click create and then lets see what happened.\n\nPublish\nYou’ll notice that GitHub Desktop is displaying two key pieces of information. First, it’s indicating that there are “No local changes” (we still haven’t done anything). Just below this, you’ll see a prominent blue button prompting you to publish your repository.\nAt this point, your repository exists only on your local machine, in the folder we created earlier. When we publish it to GitHub, the repository will be saved online, making it accessible from any device.\nNow, let’s proceed with publishing your repository.\n\n\n\n\n\nThe main option to consider here is the “Keep this code private” checkbox. This determines whether just you or everyone on GitHub will be able to see your repository. As this is just a test repository, it’s advisable to keep it private.\nIn the organization tab, you can choose whether to publish the repository in your personal account or in an organization you’re part of. For this basic tutorial, we’ll assume you’re not part of any organization yet, so you should leave this set to “None.”\nGo ahead and publish your repository. It will take a few seconds, and then you’re done. Your repository is now on GitHub! If you check it online, it should look something like this:\n\n\n\n\n\nIt may appear quite empty at first glance, but there’s an important detail to note. Below the list of files, you’ll notice a rendering of the README file. Currently, this file only contains a title (“Test”) and the simple description of our repository that we provided earlier.\n\n\n\n\n\n\nAdditional files\n\n\n\nnote that there may be additional files in your repository folder on the pc and on the github website. These are files that github auto generates and are needed to keep everything in check. As this is a basic tutorial please ignore them. They do not hurt!\n\n\nHowever, we can (and we will) modify the README file to display all the information we want to show on the main page of the repository. This README file is a powerful tool for presenting your project to visitors, and we’ll explore how to make the most of it as we progress.\n\n\nChanges\nLet’s return to our local folder and make some changes. In this example, we’ll add a Python file called example.py. The content of this file is a simple loop that prints “Such a nice repository” 1000 times. The specific code isn’t crucial; you can add any file or code you prefer.\nNow that we’ve added a file, we want our README file to reflect and explain our code. Let’s open the README file (you can use any text editor you’re comfortable with). We’ll add a line to describe our new file: The example.py file is a simple Python file that uses a loop to print “Such a nice repository” 1000 times.\n\n\n\n\n\n\nMarkdown\n\n\n\nREADME files on GitHub use Markdown, a simple syntax for formatting text. For example, to make text bold, you should enclose it in asterisks: * bold *. I won’t cover how to write in Markdown (and its different varieties), but you can check it out here.\n\n\n\n\n\n\n\n\nFolders\n\n\n\nYou can not only add files to a repository but also folders. The key advantage of adding folders, besides organizing your code better, is that each folder can have its own README file. This allows for more detailed explanations of the code and purpose of each section.\n\n\n\n\nCommit\nAfter making these changes, let’s return to GitHub Desktop. You’ll notice that GitHub Desktop displays the files with changes on the left side of the app. The README file is marked with a yellow icon because it was modified, while the Python file is green since it’s a new file. If we had removed a file, it would be marked with a red icon.\nOn the right we can see the actual changes.\n\n\n\n\n\nThe most important section isn’t the list of changes but the bottom left part, where we can add a summary (mandatory!) and a short description of the changes. For example, in the summary, we could write “Adding Python file” and in the description, “Python file looping 1000 times and printing ‘Such a nice repository’.” Once that’s done, we can press Commit to main.\nBut what does Commit mean? A commit is one of the three core actions when using Git and GitHub. Committing your changes means saving them and preparing them to be part of the repository, but it doesn’t immediately affect the remote repository (the one on GitHub). Instead, the changes are recorded locally on your machine.\nCommits are a great way to keep track of your changes. Whenever you modify your code for a specific reason, it’s a good practice to commit those changes with a clear summary and description. This helps in tracking changes over time and makes it easier to revert to previous versions if needed (we’ll cover more on that later).\n\n\n\n\n\n\nNote\n\n\n\nNote that commits can be stacked, meaning you can make changes, commit, then make more changes and commit again, and so on. You have flexibility in how often you want to commit. You don’t want so many that you get lost in useless summaries, but commit often enough to highlight specific changes clearly.\n\n\n\n\nPush\nWe have now registered (committed) our changes, but they only exist on our local machine. To add them to our GitHub repository, GitHub Desktop suggests we “Push origin” with a big blue button.\nWhat does “push” mean? This is the second core action when using Git and GitHub. Pushing sends your committed changes from your local machine to the remote repository on GitHub, making them available for others (or yourself) to access from anywhere. Ok, let’s try! Push you commits to Github. It will just take 1 second and tada!! your update code is on github.\n\n\nPull\nPull is the third and last core action when using Git and GitHub. Pulling updates your local repository with the changes that have been made on the remote repository (the one on GitHub).\nFor example, imagine you accessed your repository from another computer, made changes, and pushed those changes to GitHub. Now, on a different machine, your local repository isn’t up to date with those changes. By pulling from GitHub, you download and integrate the changes into your local repository, ensuring you can continue working from where you left off.\n\n\n\n\n\n\nTip\n\n\n\nIt’s possible that you’re not the only one updating the GitHub repository. If you’re collaborating with others, they may have made changes that you don’t yet have on your local machine. In that case, you would need to pull those changes to ensure your local repository stays in sync with the remote repository and includes all updates made by your collaborators.\n\n\nFor the sake of this tutorial, I went online to my Test repository and added another line to the README (“The code is complex”). I committed the change directly online (no need to push, since the change was made in the remote repository). Now, GitHub Desktop is notifying me that the remote repository has changes that my local folder does not have. By pressing Pull, my local folder will be updated with these changes!\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou may need to press Fetch origin in GitHub Desktop to check if there are any changes in the remote GitHub repository that are not yet in your local repository. This action updates GitHub Desktop with the latest information from the remote repository, letting you know if there are any changes that need to be pulled.\n\n\n\n\nReset Commit\nYou might be wondering, Isn’t this more complex than services like OneDrive or Google Drive, where you just add code to a folder and it syncs between devices? Well, yes and no!\nGitHub offers several advantages over simply syncing files with those other services. First and foremost, GitHub is fantastic for sharing and collaborating on the same code with multiple people because:…\n\nVersion control: GitHub tracks every change made to your files, allowing you to revert to previous versions, view history, and compare changes.\nBranching and merging: You can create separate branches to work on features or fixes without disturbing the main code, then merge them back when they’re ready.\nCollaboration tools: GitHub makes it easy to collaborate with others through pull requests, code reviews, and issue tracking, keeping the workflow organized.\n\nThose are all amazing reasons, but to be honest, if you’re just starting out, many of them won’t apply to you right away! However, there is one advantage you can leverage even now—version control.\nAs we mentioned earlier, GitHub tracks all the changes you commit to your repository. This tracking isn’t just for show! You can actually go back to a specific commit, for example, to undo code changes that might have been wrong or problematic. This ability to revert to an earlier version of your code can save you a lot of time and headaches, even as a beginner!\nGo to GitHub Desktop and click on the History section on the left side. Find the commit you want to remove, right-click on it, and select Revert changes in commit. In our case, we want to remove the previous commit we pulled from GitHub that added the line saying “The code is complex,” and we want to undo that change!\n\n\n\n\n\nTada! We’ve removed the change! Now, if you want to update the GitHub repository as well (not just the local one), simply push the new changes online. This will ensure that the remote repository reflects the undoing of the previous commit.",
    "crumbs": [
      "General",
      "Github Desktop"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "I am a Postdoc at the Centre for Brain and Cognitive Development, Birkbeck Univer- sity, studying how young minds learn to predict events and interact with their environment. My research combines behavioural experiments, eye-tracking, brain imaging, and computational mod- elling to explore the development of children’s predictive abilities and strategies for navigating the world. In addition, I share my knowledge through DevStart, a website I created with my colleagues to offer simple tutorials and insights on developmental science.\n \n  \n   \n  \n      CV\n  \n  \n    \n     Github\n  \n  \n    \n     Twitter\n  \n  \n    \n     Linkedin\n  \n  \n      Orcid\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "Curriculum\n\nAcademia formattedIndustry formatted\n\n\n\n    \n    \n    Centering an Iframe\n    \n\n\n    \n\n\n\n\n    \n    \n    Centering an Iframe\n    \n\n\n    \n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Listing.html",
    "href": "Listing.html",
    "title": "Tutorials",
    "section": "",
    "text": "This is my attempt to put together some tutorials that I think might be useful for others.\n\n\n\n\nGeneral\n\n\n\n\n\n\n\n\n\n\nGithub Desktop\n\n\n\n\n\n\nGithub\n\n\nInstallation\n\n\nGetting started\n\n\n\nVery short tutorial on how to interact with github using github desktop.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Python\n\n\n\n\n\n\nPython\n\n\nInstallation\n\n\nGetting started\n\n\n\nVery short tutorial on how I like to install and manage python. In this sort post we will see how to isntall and manage python using miniforge.\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear mixed effect modesl\n\n\n\n\n\n\nR\n\n\nStats\n\n\nMixed effects models\n\n\n\nVery short tutorial on how to what are and hwo to run Linear mixed effects models.\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/general/InstallPython.html",
    "href": "posts/general/InstallPython.html",
    "title": "Install Python",
    "section": "",
    "text": "Python is one of the most popular programming languages out there. I often use Python in our daily work to clean up and analyze data. This tutorial will show you my favorite way to install Python and manage its libraries. There are a few different ways to get Python on your computer, but this is the method I recommend for keeping things simple yet flexible.",
    "crumbs": [
      "General",
      "Install Python"
    ]
  },
  {
    "objectID": "posts/general/InstallPython.html#miniforge",
    "href": "posts/general/InstallPython.html#miniforge",
    "title": "Install Python",
    "section": "Miniforge",
    "text": "Miniforge\n\nOk, lead to Miniforge and download the version right for your system. And start the installation process:\n\n\n\n\n\n\nCaution\n\n\n\nThere are multiple version you can download. The one you should download is at the end of the page: Miniforge3\n\n\n\n\n\n\n\nOnce you press next and accept the conditions the installation program will ask you few questions like so:\n\n\n\n\n\nI suggest to keep the same options I selected but feel free to change them if you know what you are doing.\nPerfect press Install and wait for few minutes for the process to finish!! You are done!! What you should have on your system now is this icon:\n\n\n\n\n\n\n\nMiniforge Prompt\n\n\nNow I will show you how to use it!!",
    "crumbs": [
      "General",
      "Install Python"
    ]
  },
  {
    "objectID": "posts/general/InstallPython.html#use-the-miniforge-prompt",
    "href": "posts/general/InstallPython.html#use-the-miniforge-prompt",
    "title": "Install Python",
    "section": "Use the Miniforge Prompt",
    "text": "Use the Miniforge Prompt\nNow that you have Miniforge installed and can access its prompt, let’s explore how to use it effectively. Our first step will be to create a new virtual environment. What’s a virtual environment you ask?? Well python has different versions. Sometimes when we create scripts we needs to run a specific version of python because some of our packages may rely on older version of python or we have conflicting dependencies with other packages. Virtual environments keep these dependencies in separate “sandboxes” so you can switch between both applications easily and get them running.\nWhen you open Miniforge, you’ll be presented with a terminal prompt. To create your first environment, simply enter the following command:\nmamba create -n working\nThis will create a new environment called (-n) working.\nOnce the environment is created, the prompt will inform you that to work in this specific environment, you need to activate it by typing:\nmamba activate working\nYou’ll notice the prompt changes from (base) to (working), indicating which environment is currently active. Now that you’re in the correct environment, you can install the Python packages you need using mamba install followed by the package name. For example:\nmamba install pandas\n\n\n\n\n\n\nTip\n\n\n\nyou can install multiple packages at once by adding them one after the other e.g. mamba install pandas numpy matplotlib\n\n\nAfter a brief moment, the prompt will download the package(s) and their dependencies. It may ask you to confirm the installation by pressing y. A few seconds later, your chosen libraries will be ready for use! This process ensures that you have a clean, isolated environment with exactly the packages you need for your project. Perfect!\n\n\n\n\n\n\n\n\n\n\n\nMamba and Conda\n\n\n\nIn Miniforge, you have the flexibility to use both mamba and conda commands. In fact, for most operations like creating environments, activating them, and managing your setup, you can use these commands interchangeably. For example:\n mamba create -n myenv\n # or\n conda create -n myenv\n\n mamba activate myenv\n # or\n conda activate myenv\nBoth will work similarly. However, when it comes to installing packages, we recommend using mamba. The reason is simple: mamba is significantly faster at resolving dependencies and installing packages. So, for the best performance, especially with complex environments or large packages, use:\n mamba install package_name\n\n\n\nPip\nWhile conda-forge is an amazing channel to retrieve your packages not all of them are there…In such cases, pip, probably the most renowned Python package manager, can come to the rescue. You might already be aware of pip, versatility for installing and manage packages in Python. Let’s consider numpy, an exceptional package for array manipulation. With pip, installation is a breeze. Simply use the following command:\npip install numpy",
    "crumbs": [
      "General",
      "Install Python"
    ]
  },
  {
    "objectID": "posts/general/InstallPython.html#spyder",
    "href": "posts/general/InstallPython.html#spyder",
    "title": "Install Python",
    "section": "Spyder",
    "text": "Spyder\n\n\nSpyder is a powerful scientific environment IDE written in Python. Spyder is designed by and for scientists, engineers and data analysts. It integrates very useful functionalities (variable explorer, plotting, etc.) making it our favorite way to write and run python code.\n\n\nCommand prompt\nWhen you install the full version of Anaconda, it installs Spyder for you. However, Miniforge/Miniconda and do not come with Spyder. Fortunately, they do make it easy to install it. In the Miniforge Prompt activate the environment you want Spyder into and then type following command and press the “enter” key:\nmamba install spyder\nAfter thinking a bit, the Anaconda prompt will prepared a bunch of files to download and ask if you’d like to proceed. Go ahead and type “y” and hit “enter” to proceed.\nYou now should have new icon of Spyder called Spyder 6 ( “your env name” ). For example :\n\n\n\n\n\n\n\nSpyder 6 (working)\n\n\nIn case you do not see it…Don’t panic!!! You can always open the miniforge prompt, activate your environment and type spyder. Tada !! done!\n\n\n.exe\nSpyder offers an alternative installation method using a standalone file. Simply download the appropriate installation file for your operating system and run it. This approach may seem advantageous as it allows you to install Spyder IDE once, rather than in each environment. You can then switch between different environments through the graphical user interface.\nHowever, this method isn’t as straightforward as it might appear. Any environment you wish to use with Spyder still requires the spyder-kernel package. When you attempt to use Spyder in an environment lacking the spyder-kernel, it will prompt you to install it. You can do this by running mamba install spyder-kernel in the desired environment.\nUltimately, the choice between installation methods depends on your specific needs and preferences.",
    "crumbs": [
      "General",
      "Install Python"
    ]
  },
  {
    "objectID": "posts/general/InstallPython.html#positron",
    "href": "posts/general/InstallPython.html#positron",
    "title": "Install Python",
    "section": "Positron",
    "text": "Positron\n\n\nOne other IDE that I’ve recently discovered and been enjoying is Positron. Positron is a next-generation data science IDE. It’s still in active development, so some features may not work perfectly or might change in the future. However, it’s now in public beta, which means you’re welcome to give it a try! The interface and functionality of Positron feel like a hybrid between VS Code and RStudio/Spyder. Given its ongoing development, you might prefer to wait for the official release. That said, it’s already quite functional and supports both Python and R, with the ability to switch between multiple environments.\nIf you have some time, consider trying Positron out. Your experience could be valuable - don’t hesitate to submit bug reports or feature requests to help improve this promising tool.",
    "crumbs": [
      "General",
      "Install Python"
    ]
  }
]