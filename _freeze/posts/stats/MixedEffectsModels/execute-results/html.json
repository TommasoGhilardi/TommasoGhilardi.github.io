{
  "hash": "a136fe3dec5ca3dcd0f7b3fd5c90243b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear mixed effect modesl\"\ndescription: Very short tutorial on how to what are and hwo to run Linear mixed effects models.\nauthor-meta: \"Tommaso Ghilardi\"\n\ncategories:\n  - R\n  - Stats\n  - Mixed effects models\n---\n\n```{=html}\n<div style=\"display: flex; align-items: center;\">\n    <div style=\"flex: 1; text-align: left;\">\n        <p>Welcome to this introduction to Linear Mixed-effects Models (LMM)!! In this tutorial we will use R to run some simple LMM and we will try to understand together how to leverage these model for our analysis.\n        LMMs are amazing tools that have saved our asses countless times during our PhDs and Postdocs. They'll probably continue to be our trusty companions forever.</p>\n    </div>\n    <div style=\"flex: 0 0 auto; margin-left: 10px;\">\n        <iframe src=\"https://giphy.com/embed/ygCtKUnKEW5F6LruQd\" width=\"100\" height=\"100\" frameborder=\"0\" allowfullscreen></iframe>\n        <p style=\"margin: 0;\"><a href=\"https://giphy.com/gifs/TheBearFX-ygCtKUnKEW5F6LruQd\"></a></p>\n    </div>\n</div>\n```\n\n\n\n\n::: callout-note\nPlease bear in mind that this tutorial is designed to be a gentle introduction to running linear mixed-effects models. It will not delve into the mathematics and statistics behind LMM. For those interested in these aspects, numerous online resources are available for further exploration.\n:::\n\nThis tutorial introduces the statistical concept of Hierarchical Modeling, often called Mixed Effects Modeling. This approach shines when dealing with nested data‚Äîsituations where observations are grouped in meaningful ways, like students within schools or measurements across individuals.\n\nSounds like a mouthful, right? Don‚Äôt worry! Let‚Äôs kick things off with something a little more fun: Simpson‚Äôs Paradox.\n\nSimpson's Paradox is a statistical head-scratcher. It‚Äôs when a trend you see in your data suddenly flips‚Äîor even vanishes‚Äîwhen you split the data into groups. Think of it as the ‚Äúoops‚Äù moment in data analysis when you realize you missed something big. Ready to see it in action? Let‚Äôs dive in!\n\nImagine we‚Äôre looking at how years of experience impact salary at a university. Here‚Äôs some simulated data to make it fun.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(easystats)\nlibrary(tidyverse)\nlibrary(patchwork)\nset.seed(1234)\ndata <- simulate_simpson(n = 10, groups = 5, r = 0.5,difference = 1.5) %>% \n  mutate(V2= (V2 +abs(min(V2)))*10000) %>% \n  rename(Department = Group)\n\n# Lookup vector: map old values to new ones\nlookup <- c(G_1 = \"Informatics\", G_2 = \"English\", \n            G_3 = \"Sociology\", G_4 = \"Biology\", G_5 = \"Statistics\")\n\n# Replace values using the lookup vector\ndata$Department <- lookup[as.character(data$Department)]\n\n\none = ggplot(data, aes(x = V1, y = V2)) +\n  geom_point()+\n  geom_smooth(method = 'lm')+\n  labs(y='Salary', x='Year of experience', title = \"A. Linear model\")+\n  theme_bw(base_size = 20)\n\ntwo = ggplot(data, aes(x = V1, y = V2)) +\n  geom_point(aes(color = Department)) +\n  geom_smooth(aes(color = Department), method = \"lm\", alpha = 0.3) +\n  geom_smooth(method = \"lm\", alpha = 0.3)+\n  labs(y='Salary', x='Year of experience', title = \"B. Linear model acounting for hierarchical structure\")+\n  theme_bw(base_size = 20)+\n  theme(legend.position = 'bottom')\n\n(one / two)\n```\n\n::: {.cell-output-display}\n![](MixedEffectsModels_files/figure-html/SimpsonParadox-1.png){width=960}\n:::\n:::\n\n\n\n\nTake a look at the first plot. Whoa, wait a minute‚Äîthis says the more years of experience you have, the less you get paid! What kind of backwards world is this? Before you march into HR demanding answers, let‚Äôs look a little closer.\n\nNow, check out the second plot. Once we consider the departments‚ÄîInformatics, English, Sociology, Biology, and Statistics‚Äîa different story emerges. Each department shows a positive trend between experience and salary. In other words, more experience does mean higher pay, as it should!\n\nSo what happened? The first plot ignored the hierarchical structure of the data. By lumping all departments together, it completely missed the real relationship hiding in plain sight. This is why Hierarchical Modeling is so powerful‚Äîit helps us avoid embarrassing statistical blunders like this one. It allows us to correctly analyze data with nested structures and uncover the real patterns.\n\n**Now, this example is a bit of an extreme case.** In real life, you‚Äôre less likely to find such wildly opposite effects. However, the influence of grouping structures on your analysis is very real‚Äîand often subtle. Ignoring them can lead to misleading conclusions.\n\nReady to explore how Mixed Effects Modeling helps us account for these nested structures? Let‚Äôs dive in and get hands-on!\n\n### Mixed effects models\n\nSo, why are they called **mixed effects models**? It‚Äôs because these models combine two types of effects: **fixed effects** and **random effects**....... And now you‚Äôre wondering what those are, don‚Äôt worry‚ÄîI've got you! üòÖ\n\nRemember how departments (Informatics, English, Sociology, etc.) completely changed the story about experience and salary? That‚Äôs exactly where fixed and random effects come in.\n\n-   **Fixed effects** capture the consistent, predictable trends in your data‚Äîlike the relationship between experience and salary across all departments. These are the big-picture patterns you‚Äôre curious about and want to analyze.\n\n-   **Random effects**, on the other hand, account for the variability within groups‚Äîlike how salaries differ between departments. You‚Äôre not deeply analyzing these differences, but you know they‚Äôre there, and ignoring them could mess up your results.\n\nWithout accounting for random effects, it‚Äôs like assuming every department is exactly the same‚Äîand we‚Äôve already seen how misleading that can be!\n\n**The model will estimate the fixed and random effects** but don‚Äôt worry‚Äîthe model won‚Äôt get super complex. The estimates and p-values will primarily focus on the **fixed effects**, but it will account for the **random effects** in the background to make sure the results are accurate. In other words, random effects are variables that you know contribute to the variance in your model, and you want to account for them, but you‚Äôre not directly interested in obtaining a result about each one.\n\nGot the gist? Great! Now enough with words...let‚Äôs dive into a real example and see these mixed effects in action!\n\n## Settings and data\n\nIn this section, we‚Äôll load up the libraries and the data we‚Äôll use for this tutorial. The data we'll work with is simulated to resemble real-world scenarios and you can download the dataset from here:\n\n\n\n\n```{=html}\n<a href=\"../../resources/simulation/simulated_data.csv\" download class=\"btn btn-primary\">Download data</a>\n```\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\nlibrary(lmerTest)\n\nlibrary(tidyverse)\nlibrary(easystats)\n```\n:::\n\n\n\n\nThe `lme4` package is the go-to library for running Linear Mixed Models (LMM) in R. To make your life easier, there's also the `lmerTest` package, which enhances `lme4` by allowing you to extract p-values and providing extra functions to better understand your models. In my opinion, you should always use `lmerTest` alongside `lme4`‚Äîit just makes everything smoother!\n\nTo run our Linear Mixed Effects Model, these are the key packages we'll use. On top of that, the `tidyverse` suite will help with data wrangling and visualization, while `easystats` will let us easily extract and summarize the important details from our models. Let‚Äôs get started!\n\n## The data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndb = read.csv('../../resources/simulation/simulated_data.csv')\ndb$subject_id = factor(db$subject_id) # make sure subject_id is a factor\n\nhead(db)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  subject_id condition trial_number performance\n1          1    Hammer            1    548.5242\n2          1    Hammer            2    557.3464\n3          1    Hammer            3    574.2554\n4          1    Hammer            4    589.2188\n5          1    Hammer            5    601.3519\n6          1    Hammer            6    619.4832\n```\n\n\n:::\n:::\n\n\n\n\nThis dataset contains several columns. The **performance** column holds our primary variable of interest, the performance score. The **condition** column tells us the experimental condition under which the data was collected, while **trial_number** marks the specific trial, and **subject_id** gives each participant a unique identifier.\n\nAfter reading in the data, I make sure that `subject_id` is treated as a **factor** (a categorical variable) rather than a continuous one. This is important because `subject_id` represents distinct individuals, and treating it as continuous would lead the models to interpret it as a numeric variable, which is incorrect.\n\nBefore diving into modeling, it‚Äôs always a good idea to visualize your data first. So, let‚Äôs use `ggplot2` (a package from the `tidyverse` family) to get a quick look at our data. This will help us understand the structure and spot any patterns or oddities right away. Let‚Äôs do it!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(db, aes(x= trial_number, y = performance, shape = condition))+\n  geom_point(position= position_jitter(width=0.2))+\n  theme_bw(base_size = 20)+\n  labs(y='Performance', x= '# trial')+\n  scale_x_continuous(breaks = 0:10)\n```\n\n::: {.cell-output-display}\n![](MixedEffectsModels_files/figure-html/PlotRawData-1.png){width=768}\n:::\n:::\n\n\n\n\nLooking at the plot, there seems to be a small positive trend, but no real difference between the two conditions‚Äîhammer and spoon. Also, there's definitely some weird data pattern, especially near the top. Weird, right?\n\nLet's dive in and analyze this with a simple linear model. We‚Äôll start by fitting a model that predicts the performance score based on the condition (hammer or spoon).\n\n## Linear model\n\n::: callout-warning\nThroughout this section, and in the tutorial as a whole, we‚Äôll assume you have some familiarity with linear models. You don‚Äôt need to be an expert, but we will use terminology and concepts related to linear modeling. Because linear mixed-effects models are simply an extension of linear models, if you aren‚Äôt already comfortable with them, I recommend reviewing the basics before continuing.\n:::\n\nHere we run the linear model:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear_mod = lm(performance ~ condition * trial_number, db)\nsummary(Linear_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = performance ~ condition * trial_number, data = db)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-189.83  -44.25  -16.71   63.12  172.09 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  417.333     21.357  19.541  < 2e-16 ***\nconditionSpoon                16.593     30.203   0.549    0.583    \ntrial_number                  15.184      3.442   4.411 1.79e-05 ***\nconditionSpoon:trial_number   -7.599      4.868  -1.561    0.120    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 93.79 on 176 degrees of freedom\nMultiple R-squared:  0.1354,\tAdjusted R-squared:  0.1207 \nF-statistic: 9.188 on 3 and 176 DF,  p-value: 1.113e-05\n```\n\n\n:::\n:::\n\n\n\n\nWe won‚Äôt delve into the details of the model results in this tutorial, but from the summary, it‚Äôs clear that the model reflects what we saw in the plot: performance significantly increases over time, but there‚Äôs no clear difference between conditions. Now, let‚Äôs visualize it.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nl_pred = get_datagrid(Linear_mod, by= 'trial_number')\nl_pred = bind_cols(l_pred, as.data.frame(get_predicted(l_pred, Linear_mod, ci=T)))\n\nggplot(l_pred, aes(x= trial_number, y= Predicted))+\n    geom_point(data = db, aes(y= performance, shape= condition), position= position_jitter(width=0.2))+\n    geom_line(lwd=1.5, color= 'blue')+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE), color= 'transparent', alpha=0.4)+\n    labs(y='Predicted performance', x='# trial')+\n    theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](MixedEffectsModels_files/figure-html/PlotLinearModel-1.png){width=768}\n:::\n:::\n\n\n\n\nThe linear model confirmed what we observed in the data: performance increases over time, with no clear difference between conditions. But remember, this is a tutorial about mixed-effects models, so there‚Äôs something more to explore!\n\n## Mixed Effects\n\n### Random Intercept\n\nAlright, let‚Äôs start with **Random Intercepts**! What are they? Well, the name gives it away‚Äîthey‚Äôre just intercepts‚Ä¶but with a twist! ü§î\n\nIf you recall your knowledge of linear models, you‚Äôll remember that each model has **one intercept**‚Äîthe point where the model crosses the y-axis (when x=0).\n\nBut what makes random intercepts special? They allow the model to have **different intercepts for each grouping variable**‚Äîin this case, the **subjects**. This means we‚Äôre letting the model assume that each subject may have a slightly different baseline performance.\n\nHere‚Äôs the idea:\n\n-   One person might naturally be a bit better.\n\n-   Someone else could be slightly worse.\n\n-   And me? Well, let‚Äôs just say I‚Äôm starting from rock bottom.\n\nHowever, even though we‚Äôre starting from different baselines, **the rate of improvement over trials can still be consistent across subjects**.\n\nThis approach helps us capture **variation in the starting performance**, acknowledging that people are inherently different but might still follow a similar overall pattern of improvement. It‚Äôs a simple yet powerful way to model individual differences!\n\nNow, let‚Äôs look at how to include this in our mixed model.\n\n#### Model\n\nTo run a **linear mixed-effects model**, we‚Äôll use the `lmer` function from the **lme4** package. it Functions very similarly to the `lm` function we used before: you pass a formula and a dataset, but with one important addition: specifying the **random intercept**.\n\nThe formula is nearly the same as a standard linear model, but we include `(1|subject_id)` to tell the model that each subject should have its own unique intercept. This accounts for variations in baseline performance across individuals.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nIntercept_mod =lmer(performance ~ condition * trial_number + (1|subject_id ), db)\nsummary(Intercept_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ condition * trial_number + (1 | subject_id)\n   Data: db\n\nREML criterion at convergence: 1439.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5844 -0.5223 -0.0030  0.5732  3.3163 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n subject_id (Intercept) 9531     97.63   \n Residual                132     11.49   \nNumber of obs: 180, groups:  subject_id, 9\n\nFixed effects:\n                            Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)                 417.3326    32.6469   8.0925  12.783 1.19e-06 ***\nconditionSpoon               16.5930     3.7003 168.0000   4.484 1.35e-05 ***\ntrial_number                 15.1836     0.4217 168.0000  36.006  < 2e-16 ***\nconditionSpoon:trial_number  -7.5991     0.5964 168.0000 -12.742  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cndtnS trl_nm\nconditinSpn -0.057              \ntrial_numbr -0.071  0.627       \ncndtnSpn:t_  0.050 -0.886 -0.707\n```\n\n\n:::\n:::\n\n\n\n\nWow! Now the model is showing us something **new** compared to the simple linear model. We observe an **interaction between condition and trial number**. By letting the intercept vary for each subject, the model is able to capture nuances in the data that a standard linear model might miss.\n\nTo understand this interaction, let‚Äôs plot it and see how performance changes across trials for each condition.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ni_pred = bind_cols(db, as.data.frame(get_predicted(Intercept_mod, ci=T)))\n\nggplot(i_pred, aes(x= trial_number, y= Predicted, color= subject_id, shape = condition))+\n    geom_point(data = db, aes(y= performance, color= subject_id), position= position_jitter(width=0.2))+\n    geom_line()+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, fill = subject_id),color= 'transparent', alpha=0.1)+\n    labs(y='Predicted performance', x='# trial')+\n    theme_bw(base_size = 20)+\nfacet_wrap(~condition)\n```\n\n::: {.cell-output-display}\n![](MixedEffectsModels_files/figure-html/unnamed-chunk-1-1.png){width=768}\n:::\n:::\n\n\n\n\nNow, you might be thinking, *‚ÄúThis looks interesting, but my plot is going to be a mess with all these individual estimates!‚Äù* Well, don‚Äôt worry! While what we‚Äôve plotted is how the data is modeled by our mixed-effects model, the random effects are actually used to make more accurate estimates‚Äîbut the model still returns an overall estimate.\n\nThink of it like this: the random effects allow the model to account for individual differences between subjects. But instead of just showing all the individual estimates in the plot, the model takes these individual estimates for each subject and returns the *average* of these estimates to give you a cleaner, more generalizable result.\n\nwe can plot the actual estimate of the model:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ni_pred = get_datagrid(Intercept_mod, include_random = F)\ni_pred = bind_cols(i_pred, as.data.frame(get_predicted(i_pred, Intercept_mod, ci=T)))\n\nggplot(i_pred, aes(x= trial_number, y= Predicted))+\n    geom_point(data = db, aes(y= performance, color= subject_id, shape = condition), position= position_jitter(width=0.2))+\n    geom_line(aes(group= condition),color= 'blue')+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, group= condition),color= 'transparent', alpha=0.1)+\n    labs(y='Predicted performance', x='# trial')+\n    theme_bw(base_size = 20)+\n  facet_wrap(~condition)\n```\n\n::: {.cell-output-display}\n![](MixedEffectsModels_files/figure-html/PlotInterceptModelOverall-1.png){width=768}\n:::\n:::\n\n\n\n\n### Slope\n\nCoool!!!!!!! So far, we‚Äôve modeled a different intercept for each subject, which lets each subject have their own baseline level of performance. But here‚Äôs the catch: our model assumes that everyone improves over the trials in exactly the same way, with the same slope. That doesn‚Äôt sound quite right, does it? We know that some people may get better faster than others, or their learning might follow a different pattern.\n\n#### Model\n\nThis is where we can model *random slopes* to capture these individual differences in learning rates. By adding `(0 + trial_number | subject_id)`, we‚Äôre telling the model that while the intercept (starting point) is the same for everyone, the rate at which each subject improves (the slope) can vary.\n\nThis way, we‚Äôre allowing each subject to have their own slope in addition to their own intercept, making the model more flexible and reflective of real-world variations in learning!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSlope_mod =lmer(performance ~ condition * trial_number + (0+trial_number|subject_id ), db)\nsummary(Slope_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ condition * trial_number + (0 + trial_number |  \n    subject_id)\n   Data: db\n\nREML criterion at convergence: 1882.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1805 -0.4502 -0.0398  0.3804  2.9272 \n\nRandom effects:\n Groups     Name         Variance Std.Dev.\n subject_id trial_number  198.1   14.07   \n Residual                1862.7   43.16   \nNumber of obs: 180, groups:  subject_id, 9\n\nFixed effects:\n                            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)                  417.333      9.828 168.000  42.464  < 2e-16 ***\nconditionSpoon                16.593     13.899 168.000   1.194 0.234217    \ntrial_number                  15.184      4.952   9.685   3.066 0.012353 *  \nconditionSpoon:trial_number   -7.599      2.240 168.000  -3.392 0.000864 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cndtnS trl_nm\nconditinSpn -0.707              \ntrial_numbr -0.284  0.200       \ncndtnSpn:t_  0.627 -0.886 -0.226\n```\n\n\n:::\n:::\n\n\n\n\nThe results aren't too different from the intercept-only model, but let's take a closer look at what we've actually modeled.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ns_pred = bind_cols(db, as.data.frame(get_predicted(Slope_mod, ci=T)))\n\nggplot(s_pred, aes(x= trial_number, y= Predicted, color= subject_id, shape = condition))+\n    geom_point(data = db, aes(y= performance, color= subject_id), position= position_jitter(width=0.2))+\n    geom_line()+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, fill = subject_id),color= 'transparent', alpha=0.1)+\n    labs(y='Predicted performance', x='# trial')+\n    theme_bw(base_size = 20)+\nfacet_wrap(~condition)\n```\n\n::: {.cell-output-display}\n![](MixedEffectsModels_files/figure-html/PlotSlopeModel-1.png){width=768}\n:::\n:::\n\n\n\n\n### Intercept + Slope\n\nThat plot does look nuts, and it‚Äôs a clear signal that something is off. Why? Because by modeling only the random slopes while keeping the intercepts fixed, we‚Äôre essentially forcing all subjects to start from the same baseline. That‚Äôs clearly unrealistic for most real-world data.\n\nIn real life, the intercept and slope often go hand-in-hand for each subject.\n\n#### Model\n\nTo make the model more realistic, we can model both the random intercept and the random slope together. We simply modify the random effects part of the formula to `(trial_number | subject_id)`.\n\nNow, we are telling the model to estimate both a random intercept (baseline performance) and a random slope (rate of improvement). This captures the full variability in how each subject learns over time!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nInterceptSlope_mod =lmer(performance ~ condition * trial_number + (trial_number|subject_id ), db)\nsummary(InterceptSlope_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ condition * trial_number + (trial_number | subject_id)\n   Data: db\n\nREML criterion at convergence: 1198.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0164 -0.5921  0.0796  0.5935  2.8312 \n\nRandom effects:\n Groups     Name         Variance Std.Dev. Corr\n subject_id (Intercept)  9002.77  94.883       \n            trial_number   13.56   3.682   0.03\n Residual                  25.52   5.052       \nNumber of obs: 180, groups:  subject_id, 9\n\nFixed effects:\n                            Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)                 417.3326    31.6486   8.0100   13.19 1.03e-06 ***\nconditionSpoon               16.5930     1.6269 160.0003   10.20  < 2e-16 ***\ntrial_number                 15.1836     1.2412   8.1812   12.23 1.53e-06 ***\nconditionSpoon:trial_number  -7.5991     0.2622 160.0003  -28.98  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cndtnS trl_nm\nconditinSpn -0.026              \ntrial_numbr  0.027  0.094       \ncndtnSpn:t_  0.023 -0.886 -0.106\n```\n\n\n:::\n:::\n\n\n\n\nNow, let‚Äôs visualize how the model is modeling the data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nis_pred = bind_cols(db, as.data.frame(get_predicted(InterceptSlope_mod, ci=T)))\n\nggplot(is_pred, aes(x= trial_number, y= Predicted, color= subject_id, shape = condition))+\n    geom_point(data = db, aes(y= performance, color= subject_id), position= position_jitter(width=0.2))+\n    geom_line()+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, fill = subject_id),color= 'transparent', alpha=0.1)+\n    labs(y='Predicted performance', x='# trial')+\n    theme_bw(base_size = 20)+\nfacet_wrap(~condition)\n```\n\n::: {.cell-output-display}\n![](MixedEffectsModels_files/figure-html/PlotInterceptSlopeModel-1.png){width=768}\n:::\n:::\n\n\n\n\nWhile is not super apparent from the data you can see that different subject have different slopes menaing that they all not grow at the same rate\n\n## Summary of mixed models\n\nNow that we've seen how to run mixed-effects models, it's time to focus on **interpreting the summary output**. While we‚Äôve been building models, we haven‚Äôt delved into what the summary actually tells us or which parts of it deserve our attention. Let‚Äôs fix that!\n\nTo start, we‚Äôll use our **final model** and inspect its summary. This will give us a chance to break it down step by step and understand the key information it provides. Here's how to check the summary:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(InterceptSlope_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: performance ~ condition * trial_number + (trial_number | subject_id)\n   Data: db\n\nREML criterion at convergence: 1198.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0164 -0.5921  0.0796  0.5935  2.8312 \n\nRandom effects:\n Groups     Name         Variance Std.Dev. Corr\n subject_id (Intercept)  9002.77  94.883       \n            trial_number   13.56   3.682   0.03\n Residual                  25.52   5.052       \nNumber of obs: 180, groups:  subject_id, 9\n\nFixed effects:\n                            Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)                 417.3326    31.6486   8.0100   13.19 1.03e-06 ***\nconditionSpoon               16.5930     1.6269 160.0003   10.20  < 2e-16 ***\ntrial_number                 15.1836     1.2412   8.1812   12.23 1.53e-06 ***\nconditionSpoon:trial_number  -7.5991     0.2622 160.0003  -28.98  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cndtnS trl_nm\nconditinSpn -0.026              \ntrial_numbr  0.027  0.094       \ncndtnSpn:t_  0.023 -0.886 -0.106\n```\n\n\n:::\n:::\n\n\n\n\nThe **Random effects** section in the model summary shows how variability is accounted for by the random effects. The **Groups** column indicates the grouping factor (e.g., subject), while the **Name** column lists the random effects (e.g., intercept and slope). The **Variance** column represents the variability for each random effect‚Äîhigher values indicate greater variation in how the effect behaves across groups. The **Std.Dev.** column is simply the standard deviation of the variance, showing the spread in the same units as the data.\n\nThe **Corr** column reflects the correlation between random effects, telling us whether different aspects of the data (e.g., intercepts and slopes) tend to move together. A negative correlation would suggest that higher intercepts (starting points) are associated with smaller slopes (slower learning rates), while a positive correlation would suggest the opposite.\n\nThe **Residual** section shows the unexplained variability after accounting for the fixed and random effects.\n\n**The key takeaway here is that random effects capture the variability in the data that can‚Äôt be explained by the fixed effects alone.** If the variance for a random effect is low, it suggests the random effect isn‚Äôt adding much to the model and may be unnecessary. On the other hand, high variance indicates that the random effect is important for capturing group-level differences and improving the model‚Äôs accuracy.\n\n## Model comparison\n\nBut how can we be sure the random effects are helping our model? One of the easiest ways is to check the variance explained by the random effects. As we said if the variance related to the random effects is too small, it probably isn‚Äôt contributing much to the model. If it‚Äôs high, it‚Äôs likely helping the model by capturing important variability in the data.\n\nAnother method is to compare the performance of different models. One of the best indices for this is the Akaike Information Criterion (AIC). AIC gives a relative measure of how well a model fits the data, while penalizing the number of parameters in the model. Lower AIC values indicate better models, as they balance goodness-of-fit with model complexity.\n\nYou can compare the AIC of different models using the following:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_performance(Linear_mod, Intercept_mod, Slope_mod, InterceptSlope_mod, metrics='AIC')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Comparison of Model Performance Indices\n\nName               |           Model |  AIC (weights)\n-----------------------------------------------------\nLinear_mod         |              lm | 2151.5 (<.001)\nIntercept_mod      | lmerModLmerTest | 1463.2 (<.001)\nSlope_mod          | lmerModLmerTest | 1913.9 (<.001)\nInterceptSlope_mod | lmerModLmerTest | 1226.0 (>.999)\n```\n\n\n:::\n:::\n\n\n\n\nAs you can see, the best model based on AIC is the one with both intercept and slope. This is a good way to check if and which random effect structure is necessary for our model.\n\n::: callout-warning\nNever decide if your random effect structure is good by just looking at p-values! P-values are not necessarily related to how well the model fits your data. Always use model comparison and fit indices like AIC to guide your decision.\n:::\n\n## Formulary\n\nIn this tutorial, we introduced linear mixed-effects models. However, these models can be far more versatile and complex than what we've just explored. The `lme4` package allows you to specify various models to suit diverse research scenarios. While we won‚Äôt dive into every possibility, here‚Äôs a handy reference for the different random effects structures you can specify\n\n\n\n\n```{=html}\n<table border=\"1\" style=\"width: 100%; border-collapse: collapse;\">\n  <thead>\n    <tr>\n      <th style=\"white-space: nowrap;\">Formula</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"white-space: nowrap;\">(1|s)</td>\n      <td>Random intercepts for unique level of the factor <code>s</code>.</td>\n    </tr>\n    <tr>\n      <td style=\"white-space: nowrap;\">(1|s) + (1|i)</td>\n      <td>Random intercepts for each unique level of <code>s</code> and for each unique level of <code>i</code>.</td>\n    </tr>\n    <tr>\n      <td style=\"white-space: nowrap;\">(1|s/i)</td>\n      <td>\n        Random intercepts for factor <code>s</code> and <code>i</code>, where the random effects for <code>i</code> are nested in <code>s</code>. \n        This expands to <code>(1|s) + (1|s:i)</code>, i.e., a random intercept for each level of <code>s</code>, and each unique combination of the levels of <code>s</code> and <code>i</code>. \n        Nested random effects are used in so-called multilevel models. For example, <code>s</code> might refer to schools, and <code>i</code> to classrooms within those schools.\n      </td>\n    </tr>\n    <tr>\n      <td style=\"white-space: nowrap;\">(a|s)</td>\n      <td>\n        Random intercepts and random slopes for <code>a</code>, for each level of <code>s</code>. Correlations between the intercept and slope effects are also estimated. \n        (Identical to <code>(a*b|s)</code>.)\n      </td>\n    </tr>\n    <tr>\n      <td style=\"white-space: nowrap;\">(a*b|s)</td>\n      <td>\n        Random intercepts and slopes for <code>a</code>, <code>b</code>, and the <code>a:b</code> interaction, for each level of <code>s</code>. \n        Correlations between all the random effects are estimated.\n      </td>\n    </tr>\n    <tr>\n      <td style=\"white-space: nowrap;\">(0+a|s)</td>\n      <td>Random slopes for <code>a</code> for each level of <code>s</code>, but no random intercepts.</td>\n    </tr>\n    <tr>\n      <td style=\"white-space: nowrap;\">(a||s)</td>\n      <td>\n        Random intercepts and random slopes for <code>a</code>, for each level of <code>s</code>, but no correlations between the random effects (i.e., they are set to 0). \n        This expands to: <code>(0+a|s) + (1|s)</code>.\n      </td>\n    </tr>\n  </tbody>\n</table>\n```\n",
    "supporting": [
      "MixedEffectsModels_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}